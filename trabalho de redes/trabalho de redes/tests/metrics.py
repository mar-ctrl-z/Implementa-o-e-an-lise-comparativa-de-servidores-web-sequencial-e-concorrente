#!/usr/bin/env python3
"""
M√©tricas de Performance para Servidores Web
Define m√©tricas matem√°ticas para avalia√ß√£o de desempenho dos servidores sequencial e concorrente
"""

import time
import statistics
import math
from typing import List, Dict, Any
from dataclasses import dataclass
from datetime import datetime

@dataclass
class RequestMetrics:
    """M√©tricas de uma requisi√ß√£o individual"""
    request_id: int
    start_time: float
    end_time: float
    response_time: float  # Tempo de resposta (lat√™ncia)
    status_code: int
    success: bool
    server_type: str  # 'sequential' ou 'concurrent'

    @property
    def latency_ms(self) -> float:
        """Lat√™ncia em milissegundos"""
        return self.response_time * 1000

class PerformanceMetrics:
    """
    Calculadora de m√©tricas de performance para servidores web

    M√©tricas definidas conforme requisitos:
    - Lat√™ncia m√©dia (Mean Latency)
    - Throughput (Requisi√ß√µes por segundo)
    - Tempo de servi√ßo (Service Time)
    - Taxa de erro (Error Rate)
    - Utiliza√ß√£o do servidor (Server Utilization)
    """

    def __init__(self):
        self.requests: List[RequestMetrics] = []
        self.test_start_time: float = 0
        self.test_end_time: float = 0

    def add_request(self, request: RequestMetrics):
        """Adiciona uma requisi√ß√£o √†s m√©tricas"""
        self.requests.append(request)

    def start_test(self):
        """Marca o in√≠cio do teste"""
        self.test_start_time = time.time()

    def end_test(self):
        """Marca o fim do teste"""
        self.test_end_time = time.time()

    @property
    def test_duration(self) -> float:
        """Dura√ß√£o total do teste em segundos"""
        if self.test_end_time > self.test_start_time:
            return self.test_end_time - self.test_start_time
        return time.time() - self.test_start_time

    def calculate_metrics(self) -> Dict[str, Any]:
        """
        Calcula todas as m√©tricas de performance

        Returns:
            Dict contendo todas as m√©tricas calculadas
        """
        if not self.requests:
            return self._empty_metrics()

        # M√©tricas b√°sicas
        total_requests = len(self.requests)
        successful_requests = len([r for r in self.requests if r.success])
        failed_requests = total_requests - successful_requests

        # Lat√™ncias de todas as requisi√ß√µes
        latencies = [r.response_time for r in self.requests]
        successful_latencies = [r.response_time for r in self.requests if r.success]

        # C√°lculos estat√≠sticos
        metrics = {
            # === M√âTRICAS B√ÅSICAS ===
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "failed_requests": failed_requests,
            "test_duration_seconds": self.test_duration,

            # === TAXA DE SUCESSO ===
            "success_rate": successful_requests / total_requests if total_requests > 0 else 0,
            "error_rate": failed_requests / total_requests if total_requests > 0 else 0,

            # === LAT√äNCIA (RESPONSE TIME) ===
            # Lat√™ncia m√©dia: Œº = (1/n) * Œ£(response_time_i)
            "mean_latency_seconds": statistics.mean(latencies) if latencies else 0,
            "mean_latency_ms": statistics.mean(latencies) * 1000 if latencies else 0,

            # Lat√™ncia mediana
            "median_latency_seconds": statistics.median(latencies) if latencies else 0,
            "median_latency_ms": statistics.median(latencies) * 1000 if latencies else 0,

            # Desvio padr√£o da lat√™ncia: œÉ = sqrt( (1/(n-1)) * Œ£((x_i - Œº)¬≤) )
            "latency_std_seconds": statistics.stdev(latencies) if len(latencies) > 1 else 0,
            "latency_std_ms": statistics.stdev(latencies) * 1000 if len(latencies) > 1 else 0,

            # Lat√™ncia m√≠nima e m√°xima
            "min_latency_seconds": min(latencies) if latencies else 0,
            "min_latency_ms": min(latencies) * 1000 if latencies else 0,
            "max_latency_seconds": max(latencies) if latencies else 0,
            "max_latency_ms": max(latencies) * 1000 if latencies else 0,

            # Percentis (importantes para an√°lise de performance)
            "latency_p95_seconds": statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies) if latencies else 0,  # 95th percentile
            "latency_p99_seconds": statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max(latencies) if latencies else 0,  # 99th percentile

            # === THROUGHPUT ===
            # Throughput: Œª = total_requests / test_duration
            "throughput_rps": total_requests / self.test_duration if self.test_duration > 0 else 0,  # requests per second
            "throughput_rpm": (total_requests / self.test_duration) * 60 if self.test_duration > 0 else 0,  # requests per minute

            # Throughput de sucesso
            "successful_throughput_rps": successful_requests / self.test_duration if self.test_duration > 0 else 0,

            # === TEMPO DE SERVI√áO ===
            # Service Time: tempo que o servidor gasta processando requisi√ß√µes
            # Para este projeto, service time ‚âà response time (j√° que n√£o temos medi√ß√£o separada)
            "mean_service_time_seconds": statistics.mean(successful_latencies) if successful_latencies else 0,
            "service_time_std_seconds": statistics.stdev(successful_latencies) if len(successful_latencies) > 1 else 0,

            # === UTILIZA√á√ÉO DO SERVIDOR ===
            # Server Utilization: U = (service_time * arrival_rate) / num_servers
            # Para servidor sequencial: U = service_time / inter_arrival_time m√©dio
            # Para servidor concorrente: U = (service_time * Œª) / num_threads

            # === CONFIABILIDADE ===
            # Coefficient of Variation (CoV) da lat√™ncia: CoV = œÉ/Œº
            "latency_cov": (statistics.stdev(latencies) / statistics.mean(latencies)) if latencies and statistics.mean(latencies) > 0 else 0,

            # === EFICI√äNCIA ===
            # Efficiency: E = throughput / (mean_latency * num_servers)
            # Maior efici√™ncia = melhor distribui√ß√£o de carga

            # === TIMESTAMP DA AN√ÅLISE ===
            "analysis_timestamp": datetime.now().isoformat(),
            "server_type": self.requests[0].server_type if self.requests else "unknown"
        }

        # C√°lculos adicionais que dependem de outras m√©tricas
        metrics.update(self._calculate_advanced_metrics(metrics))

        return metrics

    def _calculate_advanced_metrics(self, base_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Calcula m√©tricas avan√ßadas baseadas nas m√©tricas b√°sicas"""
        advanced = {}

        # Efici√™ncia do servidor
        mean_latency = base_metrics["mean_latency_seconds"]
        throughput = base_metrics["throughput_rps"]

        if mean_latency > 0:
            # Efficiency: quanto maior, melhor (mais requisi√ß√µes por unidade de lat√™ncia)
            advanced["server_efficiency"] = throughput / mean_latency
        else:
            advanced["server_efficiency"] = 0

        # Taxa de processamento (requests per second por segundo de lat√™ncia m√©dia)
        advanced["processing_rate"] = throughput * (1 / mean_latency) if mean_latency > 0 else 0

        # √çndice de performance (throughput / lat√™ncia)
        advanced["performance_index"] = throughput / base_metrics["mean_latency_ms"] if base_metrics["mean_latency_ms"] > 0 else 0

        return advanced

    def _empty_metrics(self) -> Dict[str, Any]:
        """Retorna m√©tricas vazias para quando n√£o h√° dados"""
        return {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "test_duration_seconds": 0,
            "success_rate": 0,
            "error_rate": 0,
            "mean_latency_seconds": 0,
            "mean_latency_ms": 0,
            "throughput_rps": 0,
            "server_type": "unknown",
            "analysis_timestamp": datetime.now().isoformat()
        }

    def get_summary_report(self) -> str:
        """
        Gera um relat√≥rio resumido das m√©tricas em formato texto

        Returns:
            String com relat√≥rio formatado
        """
        metrics = self.calculate_metrics()

        report = f"""
{'='*60}
RELAT√ìRIO DE PERFORMANCE - {metrics['server_type'].upper()}
{'='*60}

üìä M√âTRICAS GERAIS:
   ‚Ä¢ Total de requisi√ß√µes: {metrics['total_requests']}
   ‚Ä¢ Requisi√ß√µes bem-sucedidas: {metrics['successful_requests']}
   ‚Ä¢ Taxa de sucesso: {metrics['success_rate']:.1%}
   ‚Ä¢ Dura√ß√£o do teste: {metrics['test_duration_seconds']:.2f}s

üèÅ LAT√äNCIA (Response Time):
   ‚Ä¢ M√©dia: {metrics['mean_latency_ms']:.2f}ms
   ‚Ä¢ Mediana: {metrics['median_latency_ms']:.2f}ms
   ‚Ä¢ Desvio padr√£o: {metrics['latency_std_ms']:.2f}ms
   ‚Ä¢ M√≠nima: {metrics['min_latency_ms']:.2f}ms
   ‚Ä¢ M√°xima: {metrics['max_latency_ms']:.2f}ms
   ‚Ä¢ 95¬∫ percentil: {metrics['latency_p95_seconds']*1000:.2f}ms

‚ö° THROUGHPUT:
   ‚Ä¢ Requisi√ß√µes/segundo: {metrics['throughput_rps']:.2f} req/s
   ‚Ä¢ Requisi√ß√µes/minuto: {metrics['throughput_rpm']:.1f} req/min
   ‚Ä¢ Throughput de sucesso: {metrics['successful_throughput_rps']:.2f} req/s

üîß TEMPO DE SERVI√áO:
   ‚Ä¢ M√©dia: {metrics['mean_service_time_seconds']*1000:.2f}ms
   ‚Ä¢ Desvio padr√£o: {metrics['service_time_std_seconds']*1000:.2f}ms

üìà EFICI√äNCIA:
   ‚Ä¢ √çndice de performance: {metrics['performance_index']:.4f}
   ‚Ä¢ Efici√™ncia do servidor: {metrics['server_efficiency']:.4f}
   ‚Ä¢ Coeficiente de varia√ß√£o: {metrics['latency_cov']:.4f}

‚è∞ AN√ÅLISE GERADA EM: {metrics['analysis_timestamp']}
{'='*60}
"""
        return report

    def export_to_json(self, filename: str):
        """Exporta m√©tricas para arquivo JSON"""
        import json
        metrics = self.calculate_metrics()

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)

        print(f"üìÑ M√©tricas exportadas para {filename}")

    def compare_with(self, other: 'PerformanceMetrics') -> Dict[str, Any]:
        """
        Compara m√©tricas com outro conjunto de m√©tricas

        Args:
            other: Outro objeto PerformanceMetrics para compara√ß√£o

        Returns:
            Dict com m√©tricas de compara√ß√£o
        """
        self_metrics = self.calculate_metrics()
        other_metrics = other.calculate_metrics()

        comparison = {
            "comparison_timestamp": datetime.now().isoformat(),
            "server_a_type": self_metrics["server_type"],
            "server_b_type": other_metrics["server_type"],
            "metrics_comparison": {}
        }

        # Comparar m√©tricas principais
        key_metrics = [
            "throughput_rps", "mean_latency_ms", "success_rate",
            "latency_std_ms", "server_efficiency", "performance_index"
        ]

        for metric in key_metrics:
            self_val = self_metrics.get(metric, 0)
            other_val = other_metrics.get(metric, 0)

            if self_val != 0 and other_val != 0:
                ratio = other_val / self_val
                improvement = ((other_val - self_val) / self_val) * 100
            else:
                ratio = 0
                improvement = 0

            comparison["metrics_comparison"][metric] = {
                "server_a_value": self_val,
                "server_b_value": other_val,
                "ratio_b_over_a": ratio,
                "improvement_percent": improvement
            }

        return comparison

# Fun√ß√µes auxiliares para an√°lise estat√≠stica
def calculate_confidence_interval(data: List[float], confidence: float = 0.95) -> tuple:
    """
    Calcula intervalo de confian√ßa para uma lista de dados

    Args:
        data: Lista de valores num√©ricos
        confidence: N√≠vel de confian√ßa (padr√£o: 95%)

    Returns:
        Tupla (lower_bound, upper_bound)
    """
    if len(data) < 2:
        return (data[0], data[0]) if data else (0, 0)

    mean = statistics.mean(data)
    std = statistics.stdev(data)
    n = len(data)

    # Valor z para confian√ßa de 95% (aproximadamente 1.96)
    z_scores = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}
    z = z_scores.get(confidence, 1.96)

    margin_error = z * (std / math.sqrt(n))

    return (mean - margin_error, mean + margin_error)

def analyze_performance_trends(metrics_list: List[PerformanceMetrics]) -> Dict[str, Any]:
    """
    Analisa tend√™ncias de performance ao longo de m√∫ltiplos testes

    Args:
        metrics_list: Lista de objetos PerformanceMetrics

    Returns:
        Dict com an√°lise de tend√™ncias
    """
    if not metrics_list:
        return {}

    throughputs = []
    latencies = []
    success_rates = []

    for m in metrics_list:
        calc_metrics = m.calculate_metrics()
        throughputs.append(calc_metrics["throughput_rps"])
        latencies.append(calc_metrics["mean_latency_ms"])
        success_rates.append(calc_metrics["success_rate"])

    return {
        "throughput_trend": {
            "mean": statistics.mean(throughputs),
            "std": statistics.stdev(throughputs) if len(throughputs) > 1 else 0,
            "min": min(throughputs),
            "max": max(throughputs),
            "confidence_interval_95": calculate_confidence_interval(throughputs)
        },
        "latency_trend": {
            "mean": statistics.mean(latencies),
            "std": statistics.stdev(latencies) if len(latencies) > 1 else 0,
            "min": min(latencies),
            "max": max(latencies),
            "confidence_interval_95": calculate_confidence_interval(latencies)
        },
        "success_rate_trend": {
            "mean": statistics.mean(success_rates),
            "std": statistics.stdev(success_rates) if len(success_rates) > 1 else 0,
            "min": min(success_rates),
            "max": max(success_rates)
        },
        "num_tests": len(metrics_list)
    }

# Teste das m√©tricas
if __name__ == "__main__":
    # Exemplo de uso
    pm = PerformanceMetrics()
    pm.start_test()

    # Simular algumas requisi√ß√µes
    for i in range(10):
        latency = 0.1 + (i * 0.01)  # Lat√™ncia crescente para teste
        req = RequestMetrics(
            request_id=i,
            start_time=time.time(),
            end_time=time.time() + latency,
            response_time=latency,
            status_code=200,
            success=True,
            server_type="sequential"
        )
        pm.add_request(req)

    pm.end_test()

    # Mostrar relat√≥rio
    print(pm.get_summary_report())

    # Exportar para JSON
    pm.export_to_json("test_metrics.json")
